{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "soviet-image",
   "metadata": {},
   "source": [
    "# Fire stats data\n",
    "\n",
    "Process raw annual severity and structural data to generate tabular data of area burned. Specifically, calculate the stratified area burned in each westside ecoregion and in LSRs in each ecoregion from 1985-2020, as well as the total are of old growth in each ecoregion and in LSRs in each ecoregion.\n",
    "\n",
    "Strata\n",
    "- Severity\n",
    "- Structural condition\n",
    "- Year\n",
    "- Ecoregion\n",
    "- State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intense-passion",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-02T19:29:25.038041Z",
     "start_time": "2021-08-02T19:29:24.539708Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import rasterio as rio\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excess-history",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accepting-techno",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-02T19:29:27.083272Z",
     "start_time": "2021-08-02T19:29:27.077288Z"
    }
   },
   "outputs": [],
   "source": [
    "TARGET_CRS = \"EPSG:5070\"\n",
    "\n",
    "NODATA = 0\n",
    "PIXEL_AREA = 30 * 30\n",
    "SQM_PER_HA = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mexican-commitment",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-02T19:29:28.303822Z",
     "start_time": "2021-08-02T19:29:28.296238Z"
    }
   },
   "outputs": [],
   "source": [
    "struc_classes = {\n",
    "    -1: \"Nonforest\",\n",
    "    1: \"Sparse\",\n",
    "    2: \"Open\",\n",
    "    3: \"Sapling/pole - moderate/closed\", \n",
    "    4: \"Small/medium tree - moderate/closed\", \n",
    "    5: \"Large tree - moderate/closed\", \n",
    "    6: \"Large/giant tree - moderate/closed\"\n",
    "}\n",
    "\n",
    "sev_classes = {\n",
    "    0: \"masked\",\n",
    "    1: \"Very low / unburned\",\n",
    "    2: \"Low\", \n",
    "    3: \"Low / moderate\",\n",
    "    4: \"Moderate / high\",\n",
    "    5: \"High\", \n",
    "    6: \"Very high\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "artificial-memphis",
   "metadata": {},
   "source": [
    "## Fire Area\n",
    "Creates a table showing the final fire areas of major and other fires, from NIFC data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sound-thong",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join(\"..\", \"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minimal-breach",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-02T19:29:34.537505Z",
     "start_time": "2021-08-02T19:29:34.072510Z"
    }
   },
   "outputs": [],
   "source": [
    "fire_path = os.path.join(data_dir, \"NIFC\", \"Public_NIFS_Perimeters_westside_ecoregions_complete.shp\")\n",
    "fires = gpd.read_file(fire_path).to_crs(TARGET_CRS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "victorian-render",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-02T19:29:37.071001Z",
     "start_time": "2021-08-02T19:29:37.033771Z"
    }
   },
   "outputs": [],
   "source": [
    "fires[\"fire_name\"] = fires.IncidentNa.str.title()\n",
    "# Exclude irrelevant fires\n",
    "exclude_fires = [\"S. Obenchain\", \"White River\", \"Grizzly Creek\"]\n",
    "fires = fires[~fires.fire_name.isin(exclude_fires)]\n",
    "\n",
    "# Calculate fire area in hectares\n",
    "fires.ha = fires.area.divide(SQM_PER_HA)\n",
    "\n",
    "fire_area_all = fires.sort_values([\"ha\"], ascending=False)[[\"fire_name\", \"ha\"]]\n",
    "\n",
    "# Fires smaller than this threshold will be grouped into \"Other\"\n",
    "other_threshold_ha = 10_000\n",
    "fires['fire_name'] = fires.apply(lambda x : \"Other\" if x.ha < other_threshold_ha else x.fire_name, axis=1)\n",
    "\n",
    "# Group and sum the \"Other\" fires\n",
    "fires = fires.groupby('fire_name').sum().reset_index()\n",
    "\n",
    "fire_area_other = fires.sort_values([\"ha\"], ascending=False)[[\"fire_name\", \"ha\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "friendly-pharmacology",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-22T18:44:50.045864Z",
     "start_time": "2021-06-22T18:44:50.030680Z"
    }
   },
   "outputs": [],
   "source": [
    "fire_area_all.to_csv(\"fire_area_all.csv\", index=False)\n",
    "fire_area_other.to_csv(\"fire_area_other.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reliable-basics",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "improved-watershed",
   "metadata": {},
   "source": [
    "### Ecoregion data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "athletic-bulgaria",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-02T19:30:59.883981Z",
     "start_time": "2021-08-02T19:30:59.541128Z"
    }
   },
   "outputs": [],
   "source": [
    "eco_path = os.path.join(data_dir, \"study_area.gpkg\")\n",
    "eco = gpd.read_file(eco_path).to_crs(TARGET_CRS)\n",
    "eco.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "native-designer",
   "metadata": {},
   "source": [
    "### Severity and struccond data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fatal-matrix",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-02T19:31:02.896296Z",
     "start_time": "2021-08-02T19:31:02.882187Z"
    }
   },
   "outputs": [],
   "source": [
    "# Available https://lemma.forestry.oregonstate.edu/data/structure-maps\n",
    "struc_dir = os.path.join(data_dir, \"struccond\")\n",
    "# The severity grids can be generated using `scripts/severity.js`\n",
    "sev_dir = os.path.join(data_dir, \"severity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "departmental-venezuela",
   "metadata": {},
   "source": [
    "### LSR data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "julian-budapest",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-02T19:31:09.177208Z",
     "start_time": "2021-08-02T19:31:07.009077Z"
    }
   },
   "outputs": [],
   "source": [
    "# LSR polygons pre-clipped to the Westside ecoregions\n",
    "lsr_path = os.path.join(\"..\", \"data\", \"boundary\", \"LSR_westside_ecoregions_dissolve.shp\")\n",
    "lsr = gpd.read_file(lsr_path).to_crs(TARGET_CRS)\n",
    "\n",
    "# Clip the ecoregions to the LSRs so that LSRs can be iterated by ecoregion.\n",
    "eco_lsr = gpd.clip(eco, lsr, keep_geom_type=True)\n",
    "\n",
    "eco_lsr.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "threaded-auditor",
   "metadata": {},
   "source": [
    "## Ecoregions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ready-naples",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-02T19:31:18.161020Z",
     "start_time": "2021-08-02T19:31:18.143530Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_sev_distribution(struc_arr, sev_arr, year, ecoregion, state):\n",
    "    \"\"\"\n",
    "    Return a dataframe of the distribution of burn severity pixels, stratified by structural class.\n",
    "    \"\"\"\n",
    "    df_list = []\n",
    "    \n",
    "    # For each structural condition class\n",
    "    for struc_level, struc_label in struc_classes.items():\n",
    "        # Mask all but one structural class\n",
    "        struc_mask = np.where(struc_arr == struc_level, 1, 0)\n",
    "        sev_masked = sev_arr * struc_mask\n",
    "\n",
    "        # Count the occurence of every possible severity class\n",
    "        sev_table = np.bincount(sev_masked.flatten(), minlength=len(sev_classes))\n",
    "        sev_df = pd.DataFrame(sev_table, columns=[\"n_burned\"])\n",
    "        sev_df[\"hectares_burned\"] = sev_df[\"n_burned\"] * PIXEL_AREA / SQM_PER_HA\n",
    "        sev_df[\"severity\"] = sev_classes.values()\n",
    "        sev_df[\"struccond\"] = struc_label\n",
    "        sev_df[\"year\"] = year\n",
    "        sev_df[\"ecoregion\"] = ecoregion\n",
    "        sev_df[\"state\"] = state\n",
    "        # Remove masked pixels\n",
    "        sev_df = sev_df[sev_df[\"severity\"] != \"masked\"]\n",
    "\n",
    "        df_list.append(sev_df)\n",
    "\n",
    "    return pd.concat(df_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loving-immigration",
   "metadata": {},
   "source": [
    "### Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a8ffba",
   "metadata": {},
   "source": [
    "Calculate the stratified area burned in each westside ecoregion and in LSRs in each ecoregion from 1985-2020, as well as the total are of old growth in each ecoregion and in LSRs in each ecoregion.\n",
    "\n",
    "Strata\n",
    "- Severity\n",
    "- Structural condition\n",
    "- Year\n",
    "- Ecoregion\n",
    "- State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9035e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-02T19:31:19.475672Z",
     "start_time": "2021-08-02T19:31:19.468690Z"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "today = date.today().strftime(format=\"%Y%m%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5947aadf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-02T19:31:22.731754Z",
     "start_time": "2021-08-02T19:31:22.723237Z"
    }
   },
   "outputs": [],
   "source": [
    "# Reset indexes so we can index LSRs using iterrows\n",
    "eco.reset_index(drop=True, inplace=True)\n",
    "eco_lsr.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de483352",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-02T20:01:56.242639Z",
     "start_time": "2021-08-02T19:31:42.299456Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_yr_dfs = []\n",
    "working_files = []\n",
    "\n",
    "yr_range = range(1986, 2021)\n",
    "\n",
    "\n",
    "# 30 minutes to run\n",
    "for year in yr_range:\n",
    "    print(year)\n",
    "    sev_year = year\n",
    "    # Prefire conditions\n",
    "    struc_year = year - 1\n",
    "    \n",
    "    struc_path = os.path.join(struc_dir, f\"struccond_{struc_year}.tif\")\n",
    "    \n",
    "    sev_path = os.path.join(sev_dir, f\"severity_{sev_year}.tif\")\n",
    "    \n",
    "    struc = rio.open(struc_path)\n",
    "    sev = rio.open(sev_path)\n",
    "    \n",
    "    for i, row in eco.iterrows():       \n",
    "        ecoregion = row.NA_L3NAME\n",
    "        state = row.STATE_NAME\n",
    "        geom = row.geometry\n",
    "        print(f\"\\t{state} - {ecoregion}\")\n",
    "        \n",
    "        # Calculate area by structure and severity throughout the ecoregion\n",
    "        struc_arr, _ = rio.mask.mask(struc, geom, crop=True, indexes=1, nodata=NODATA)\n",
    "        sev_arr, _ = rio.mask.mask(sev, geom, crop=True, indexes=1, nodata=NODATA)\n",
    "        assert struc_arr.shape == sev_arr.shape, (struc_arr.shape, sev_arr.shape)\n",
    "        \n",
    "        region_data = get_sev_distribution(struc_arr, sev_arr, year, ecoregion, state)\n",
    "        # Store the region area to allow calculating proportion of area\n",
    "        region_data = region_data.assign(region_hectares=geom.area / SQM_PER_HA)\n",
    "\n",
    "        # Count the pixels of each structural class in the region to calculate total (not burned) OG area\n",
    "        region_val, region_counts = np.unique(struc_arr, return_counts=True)\n",
    "        struc_df = pd.DataFrame(zip(region_val, region_counts), columns=[\"struc_id\", \"n\"], index=None)\n",
    "        # Remove no data\n",
    "        struc_df = struc_df[struc_df.struc_id != 0]\n",
    "        # Calculate area in hectares using pixel counts\n",
    "        struc_df[\"hectares\"] = struc_df[\"n\"] * PIXEL_AREA / SQM_PER_HA\n",
    "        # Total hectares of old growth in westside ecoregions\n",
    "        og_hectares = struc_df[struc_df.struc_id.isin([5, 6])].hectares.sum()\n",
    "        \n",
    "        \n",
    "        # Calculate area by structure and severity within LSRs in the ecoregion\n",
    "        lsr = eco_lsr.iloc[i]\n",
    "        lsr_geom = lsr.geometry\n",
    "        \n",
    "        lsr_struc_arr, _ = rio.mask.mask(struc, lsr_geom, crop=True, indexes=1, nodata=NODATA)\n",
    "        lsr_sev_arr, _ = rio.mask.mask(sev, lsr_geom, crop=True, indexes=1, nodata=NODATA)\n",
    "        assert lsr_struc_arr.shape == lsr_sev_arr.shape, (lsr_struc_arr.shape, lsr_sev_arr.shape)\n",
    "        \n",
    "        lsr_data = get_sev_distribution(lsr_struc_arr, lsr_sev_arr, year, ecoregion, state)\n",
    "        \n",
    "        # Count the pixels of each structural class in LSRs to calculate total (not burned) LSR OG area\n",
    "        lsr_val, lsr_counts = np.unique(lsr_struc_arr, return_counts=True)\n",
    "        lsr_struc_df = pd.DataFrame(zip(lsr_val, lsr_counts), columns=[\"struc_id\", \"n\"], index=None)\n",
    "        # Remove no data\n",
    "        lsr_struc_df = lsr_struc_df[lsr_struc_df.struc_id != 0]\n",
    "        # Calculate area in hectares using pixel counts\n",
    "        lsr_struc_df[\"hectares\"] = lsr_struc_df[\"n\"] * PIXEL_AREA / SQM_PER_HA\n",
    "        # Total hectares of old growth in LSR in westside ecoregions\n",
    "        lsr_og_hectares = lsr_struc_df[lsr_struc_df.struc_id.isin([5, 6])].hectares.sum()\n",
    "        \n",
    "        \n",
    "        region_data = region_data.assign(\n",
    "            lsr_n_burned = lsr_data[\"n_burned\"],\n",
    "            lsr_hectares_burned = lsr_data[\"hectares_burned\"],\n",
    "            region_lsr_hectares = lsr_geom.area / SQM_PER_HA,\n",
    "            region_og_hectares = og_hectares,\n",
    "            region_lsr_og_hectares = lsr_og_hectares\n",
    "        )\n",
    "        \n",
    "        # Subset and re-arrange the columns\n",
    "        region_data = region_data[[\n",
    "            \"year\",\n",
    "            \"ecoregion\",\n",
    "            \"state\",\n",
    "            \"severity\",\n",
    "            \"struccond\",\n",
    "            \"hectares_burned\",\n",
    "            \"lsr_hectares_burned\",\n",
    "            \"region_hectares\",\n",
    "            \"region_lsr_hectares\",\n",
    "            \"region_og_hectares\",\n",
    "            \"region_lsr_og_hectares\"\n",
    "        ]]\n",
    "        \n",
    "        all_yr_dfs.append(region_data)\n",
    "    \n",
    "    # Save a working file in case something breaks. Store the path for cleanup once finished.\n",
    "    working_df = pd.concat(all_yr_dfs)\n",
    "    working_file = f\"working_data_{int(time.time())}.csv\"\n",
    "    working_df.to_csv(working_file, index=False)\n",
    "    working_files.append(working_file)\n",
    "        \n",
    "all_dfs = pd.concat(all_yr_dfs, ignore_index=True)\n",
    "# Assign year groups\n",
    "all_dfs = all_dfs.assign(yr_group=all_dfs.year.apply(lambda x : \"2020\" if x == 2020 else \"1985 - 2019\"))\n",
    "\n",
    "all_dfs.to_csv(f\"fire_stats_data_{today}.csv\", index=False)\n",
    "\n",
    "for file in working_files:\n",
    "    if os.path.isfile(file):\n",
    "        os.remove(file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
